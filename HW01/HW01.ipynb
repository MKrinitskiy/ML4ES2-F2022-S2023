{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ №1 - обучение модели линейной регресии методом градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать обучение модели \"нейросети\" (в варианте линейной регрессии) методом градиентного спуска.<br />\n",
    "\n",
    "В качестве подводящего упражнения в этом задании предлагается реализовать функции потерь и саму модель \"нейросети\" (линейной регрессии) в манере, схожей с построением модулей фреймворка pytorch (см. пояснения в шаблонах кода)\n",
    "\n",
    "В решении ожидается наличие следующих ключевых составляющих:<br />\n",
    "\n",
    "#### Текстовое описание в решении:\n",
    "- формулировка задачи, формулировка признакового описания объектов, формулировка функции ошибки;\n",
    "- исследование исходных данных на предмет скоррелированности признаков; фильтрация признаков; порождение признаков (при необходимости);\n",
    "- оценка параметров модели линейной регрессии (обучение модели) методом градиентного спуска;\n",
    "\n",
    "#### Код решения:\n",
    "(используйте предлагаемые шаблоны)\n",
    "- формулировка всех составляющих модели \"нейросети\";\n",
    "- формулировка модели \"нейросети\" - `NN` (линейной регрессии);\n",
    "- формулировка функции ошибки вместе с ее составляющими (например, класс отклонения `Residual`);\n",
    "- формулировка цикла оптимизации параметров.\n",
    "\n",
    "\n",
    "#### Визуализация в решении:\n",
    "- распределения признаков;\n",
    "- распределение целевой переменной;\n",
    "- эволюция функции ошибки и выбранных метрик качества по ходу обучения.\n",
    "- диаграмма соответствия измеренной целевой переменной и значений целевой переменной, оцененной с использованием обученной \"нейросети\"\n",
    "\n",
    "#### Выводы (в форме текста!)\n",
    "- вывод о том, насколько модель подходит для описания данных\n",
    "- вывод о достаточности или избыточности данных для оценки параметров модели\n",
    "- вывод о соотношении выразительности модели и ее обобщающей способности (наблюдаются ли явления переобучения или недообучения).\n",
    "\n",
    "Примечания:<br />\n",
    "Допустимо порождение признаков (полиномиальных, экспоненциальных, логарифмических, etc.)<br />\n",
    "Реализация линейной регрессии может быть написана только с использованием библиотеки Numpy. Решения с использованием библиотек автоматического вычисления градиентов не засчитываются.<br />\n",
    "Из готовых реализаций (напр., из пакета scikit-learn) в этом задании допускается использовать только порождение полиномиальных признаков `PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные находятся в следующих файлах:\n",
    "\n",
    "Признаковое описание объектов обучающей выборки - в файле X_train.npy\n",
    "\n",
    "Значения целевой переменной на обучающей выборке - в файле y_train.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Способ чтения данных из файлов *.npy :\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "data = np.load('/path/to/filename.npy')\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примечание на предмет реализации градиента функции потерь\n",
    "\n",
    "Нелишним будет вспомнить способ вычисления градиента сложной функции. Здесь функция ошибки (обозначено как $\\mathscr{L}$) представлена как сложная функция $\\mathscr{L}\\left( G\\left( \\theta \\right) \\right)$. Для простоты приведена сразу матричная запись. По ней можно сверить свой результат.\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}{\\mathscr{L}} = \\nabla_{G}{\\mathscr{L}}\\cdot\\nabla_{\\theta}{G}\n",
    "$$\n",
    "\n",
    "В качестве шпаргалки можно подсмотреть правила матричного дифференцирования <a href=\"https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\">здесь</a>\n",
    "\n",
    "Например, в случае функции потерь MSE это может выглядеть следующим образом:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}{\\mathscr{L}} = 2\\left(X\\theta - Y\\right) \\cdot \\mathbf{I} \\cdot X\n",
    "$$\n",
    "\n",
    "В этом ДЗ следует очень аккуратно реализовать градиент каждой отдельной операции по аргументу этой операции:\n",
    "- градиент квадрата отклонения $d^2$ - по отклонению $d$\n",
    "- градиент отклонения $\\left(\\hat{y}-y\\right)$ - по аргументу $\\hat{y}$\n",
    "- градиент функции активации $\\hat{y}=A\\left(z\\right)$ - по ее агрументу $z$ (да, здесь функция активации $A$ - Identity, но это сейчас не так важно)\n",
    "- градиент оценки $z=\\theta^T\\cdot X$ - по аргументу $\\theta$\n",
    "\n",
    "**ВНИМАНИЕ**\n",
    "В этом задании также следует учесть, что подразумевается, что метод `backward` каждого класса выдает градиент **функции ошибки** по аргументу операции. Для учета градиентов всех предыдущих операций в этот метод передается т.н. **upstream gradient** - переменная `usg`. Не забывайте ее передавать при \"сборке\" полного градиента. Эта \"сборка\" у вас будет в двух классах - классе функции потерь `MSE` (нужно собрать градиент операции `MSE` с учетом того, что она, в свою очередь, сложная функция, использующая `Residual`) и классе нейросети `NN`.\n",
    "\n",
    "Как можно видеть, все операции, из которых составляется \"нейросеть\" в этом задании, могут быть представлены однотипно: для всех из них можно задать метод вычисления `forward` на \"прямом проходе\" и метод вычисления градиента `backward` на этапе вычисления градиентов, \"обратном проходе\".\n",
    "\n",
    "**ВНИМАНИЕ**\n",
    "Не следует забывать, что для вычисления градиентов обычно используются результаты операции, вычисленные на этапе \"прямого прохода\". Для хранения этих результатов используйте атрибуты класса `cache`. Напомним, ссылка на сам экземпляр класса в теле метода класса обычно упоминается как `self`. То есть, атрибут `cache` этого экземпляра класса будет в этом методе упоминаться как `self.cache`. Вы можете назвать его как угодно (не обязательно именно `cache`), но реализация хранения промежуточных результатов вычисления нейросети - **обязательно** в этом ДЗ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = np.load('./X_train.npy')\n",
    "ytr = np.load('./y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xtr[:,0], ytr, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xtr[:,1], ytr, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xtr[:,2], ytr, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xtr[:,3], ytr, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(Xtr, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примите решение о фильтрации признаков или порождении новых признаков\n",
    "# Xtr = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Differentiable:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Residual, self).__init__()\n",
    "    \n",
    "    def __call__(self, mu, y):\n",
    "        return self.forward(mu, y)\n",
    "    \n",
    "    def forward(self, mu, y):\n",
    "        # Этот метод реализует вычисление отклонения mu-y\n",
    "        d = None\n",
    "        self.cache = None\n",
    "        \n",
    "        return d\n",
    "    \n",
    "    def backward(self, usg):\n",
    "        # Этот метод реализует вычисление градиента отклонения D по аргументу mu\n",
    "        \n",
    "        partial_grad = None\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # partial_grad = ...\n",
    "        \n",
    "        return partial_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(MSE, self).__init__()\n",
    "        self.diff = Residual()\n",
    "    \n",
    "    def __call__(self, mu, y):\n",
    "        # d = ...\n",
    "        return self.forward(d)\n",
    "    \n",
    "    def forward(self, d):\n",
    "        # Этот метод реализует вычисление значения функции потерь\n",
    "        # Подсказка: метод должен возвращать единственный скаляр - значение функции потерь\n",
    "        self.cache = None\n",
    "        loss_value = None\n",
    "        \n",
    "        return loss_value\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "        # Этот метод реализует вычисление градиента функции потерь по аргументу d\n",
    "        # Подсказка: метод должен возвращать вектор градиента функции потерь\n",
    "        #           размерностью, совпадающей с размерностью аргумента d\n",
    "        \n",
    "        partial_grad = None\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # partial_grad = ...\n",
    "        \n",
    "        return partial_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(linear, self).__init__()\n",
    "        self.theta = None\n",
    "        self.cache = None\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # этот метод предназначен для применения модели к данным\n",
    "        assert X.ndim == 2, \"X should be 2-dimensional: (N of objects, n of features)\"\n",
    "        \n",
    "        # ВНИМАНИЕ! Матрица объекты-признаки X не включает смещение\n",
    "        #           Вектор единиц для применения смещения нужно присоединить самостоятельно!\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # X_ = ...\n",
    "        \n",
    "        if (self.theta is None):\n",
    "            # Если вектор параметров еще не инициализирован, его следует инициализировать\n",
    "            # Подсказка: длина вектора параметров может быть получена из размера матрицы X\n",
    "            # Fx1.T dot NxF.T = 1xN\n",
    "            # Если X - матрица объекты-признаки, то это матрица из вектор-строк!\n",
    "            self.theta = None\n",
    "        \n",
    "        \n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        \n",
    "        z = None\n",
    "        self.cache = None\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # mu = ...\n",
    "        # self.cache = ...\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def backward(self, usg):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        assert self.cache is not None, \"please perform forward pass first\"\n",
    "        \n",
    "        partial_grad = None\n",
    "        self.cache = None\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # partial_grad = ...\n",
    "        \n",
    "        # Не забудьте очистить кэш!\n",
    "        # self.cache = ...\n",
    "        \n",
    "        return partial_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        # этот метод предназначен для вычисления значения функции активации\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def backward(self, usg):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        return usg\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # этот метод предназначен для вычисления функции активации\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.l1 = linear()\n",
    "        self.act = Identity()\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Этот метод будет вычислять нейросеть на данных X\n",
    "        ### YOUR CODE HERE\n",
    "        # x = ...\n",
    "        return x\n",
    "    \n",
    "    def backward(self, usg):\n",
    "        grad = None\n",
    "        ### YOUR CODE HERE\n",
    "        # grad = ...\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_by_norm(grad, max_norm = 1.0):\n",
    "    grad_norm = np.linalg.norm(grad)\n",
    "    if grad_norm > max_norm:\n",
    "        grad = max_norm * grad / grad_norm\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = network(Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = MSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(mu, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.backward(loss_fn.backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Далее идет процедура обучения созданной нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "pbar = tqdm(total=epochs)\n",
    "for epoch in range(epochs):\n",
    "    mu = None\n",
    "    loss_value = None\n",
    "    grad = None\n",
    "    grad = clip_by_norm(grad, 1.0)\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    # mu = ...\n",
    "    # loss_value = ...\n",
    "    # grad = ...\n",
    "    # grad = clip_by_norm(grad, 10)\n",
    "    \n",
    "    # update network parameters\n",
    "    # network.l1.theta = ... + ...\n",
    "    loss_history.append(loss_value)\n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix({'loss': loss_value})\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отобразите эволюцию функции потерь по мере обучения сети\n",
    "plt.plot(loss_history)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# примените нейросеть к данным Xtr\n",
    "mu = network(Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отобразите диаграмму y(y_true) для оценки соответствия полученного решения известному\n",
    "plt.scatter(ytr, mu, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
